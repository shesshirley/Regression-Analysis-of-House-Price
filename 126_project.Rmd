---
title: "126_PROJECT"
author: "Shirley Wang"
date: "2019/11/24"
output: html_document
---
```{r}
install.packages("readxl",repos = "http://cran.us.r-project.org")
library(readxl)
```

```{r}
install.packages("leaps",repos = "http://cran.us.r-project.org")
library(leaps)
```

```{r}
install.packages("MASS",repos = "http://cran.us.r-project.org")
library(MASS)
```

```{r}
RE<-read_xlsx("Real estate valuation data set.xlsx")
Date<-RE$`X1 transaction date`
House_age<-RE$`X2 house age`
Distance<-RE$`X3 distance to the nearest MRT station`
Store_number<-RE$`X4 number of convenience stores`
Latitude<-RE$`X5 latitude`
Longitude<-RE$`X6 longitude`
response<-RE$`Y house price of unit area`
#to fit the linear models
fit=lm(response~Date+House_age+Distance+Store_number+Latitude+Longitude)
#get the estimated regression equation
coef(fit)
#response=-1.444198e+04+5.149017e+00*Date-2.696967e-01*House_age-4.487508e-03*Distance+1.133325e+00*Store_number+2.254701e+02*Latitude

#scatterplot matrix
pairs(response~Date+House_age+Distance+Store_number+Latitude+Longitude)

#summary of fit
summary(fit)
yhat = fitted(fit)
e = response - yhat
#plot residuals,ei, versus fitted values/predited values to check the linearity
plot(yhat, e, xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Fit')
abline(h = 0, lty = 2)
#normal Q-Q plot to check for approximate normality
qqnorm(e)
qqline(e)
#histgram of the Residuals
hist(e)

n=length(Distance)
outlier=rstudent(fit)
for (i in 1:n){
  if (outlier[i]>3){
    print (outlier[i])
  }
}
#We can get 6 outliers here, there are point127,149,221,271,313 and 390. The 271st has the largest value, in order to improve our data, we will delete it first to find the best model.
```

```{r}
RE2<-read_xlsx("Real estate valuation data set_2.xlsx")
Date2<-RE2$`X1 transaction date`
House_age2<-RE2$`X2 house age`
Distance2<-RE2$`X3 distance to the nearest MRT station`
Store_number2<-RE2$`X4 number of convenience stores`
Latitude2<-RE2$`X5 latitude`
Longitude2<-RE2$`X6 longitude`
response2<-RE2$`Y house price of unit area`

#to fit the models
fit2=lm(response2~Date2+House_age2+Distance2+Store_number2+Latitude2+Longitude2)
summary(fit2)
yhat2=fitted(fit2)
e2=response2 - yhat2
plot(yhat2, e2, xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Fit')
abline(h = 0, lty = 2)
qqnorm(e2)
qqline(e2)
hist(e2)

#plot residuals vs fitted for each predictors
plot(Date2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Date')
plot(House_age2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs House_age')
plot(Distance2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Distance')
plot(Store_number2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Store_number')
plot(Latitude2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Latitude')
plot(Longitude2, resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Longitude')

#plot residuals vs log(predictor)
plot(log(Distance2), resid(fit2), xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Distance')

#logarithmic transformation on response
fit3=lm(log(response2)~Date2+House_age2+log(Distance2)+Store_number2+Latitude2+Longitude2)
summary(fit3)
yhat3=fitted(fit3)
e3=log(response2) - yhat3
plot(yhat3, e3, xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Fit')
abline(h = 0, lty = 2)
qqnorm(e3)
qqline(e3)
hist(e3)

#boxcox to find best lamdba value
boxcox.trans=boxcox(response2~Date2+House_age2+log(Distance2)+Store_number2+Latitude2+Longitude2,lambda = seq(0,1,length=10))
#The profile log-likelihood plot suggests that lambda=0.2 is the best value for this data set. Accordingly, the transformed response is Y^(0.2)
fit4=lm((response2)^(0.42)~Date2+House_age2+log(Distance2)+Store_number2+Latitude2+Longitude2)
summary(fit4)
yhat4=fitted(fit4)
e4=response2^(0.42) - yhat4
plot(yhat4, e4, xlab = 'Fitted Values', ylab = 'Residual', main = 'Residual vs Fit')
abline(h = 0, lty = 2)
qqnorm(e4)
qqline(e4)
hist(e4)
summary(fit4)
``` 
```{r}
#using F-tests to choose variables
mod0=lm((response2)^(0.42)~log(Distance2)+House_age2)
#check the p-value for the F-test for rest predictors
add1(mod0,~.+Date2+Store_number2+Latitude2+Longitude2,test='F')
#It is clear to see that F-statistic (p-value) of Latitude is the largest (smallest) one.
mod1=update(mod0,~.+Latitude2)
add1(mod1,~.+Date2+Store_number2+Longitude2,test='F')
#It is clear to see that F-statistic (p-value) of Date is the largest (smallest) one.
mod2=update(mod1,~.+Date2)
summary(mod2)
#The p-value for log(Distance), House_age and Latitude shows that adding Date in the model doesn't affect the significance of log(Distance), House_age and Latitude.
mod2=update(mod1,~.+Date2)
add1(mod2,~.+Store_number2+Longitude2,test='F')
#It is clear to see that F-statistic (p-value) of Store_number is the largest (smallest) one.
mod3=update(mod2,~.+Store_number2)
summary(mod3)
#The p-value for log(Distance), House_age, Latitude and Date shows that adding Store_number in the model doesn't affect the significance of log(Distance), House_age, Latitude and Date.
mod3=update(mod2,~.+Store_number2)
add1(mod3,~.+Longitude2,test='F')
#It is clear to see that p-value of Longitude is less than 0.5.

#choose predictors is Akaike's Information Criterion (AIC)
mod0=lm((response2)^(0.42)~log(Distance2)+House_age2)
mod.upper=lm((response2)^(0.42)~Date2+Store_number2+Latitude2+Longitude2+log(Distance2)+House_age2)
step(mod0,scope=list(lower=mod0,upper=mod.upper))

#we get same final model as before when using the Stepwise model method.

#Best Subsets Regression
#R packgae leaps to conduct best subset regression
mod=regsubsets(cbind(log(Distance2),Store_number2,House_age2,Latitude2,Longitude2,Date2),(response2)^(0.42))
summary.mod=summary(mod)
#show that a "best" model includes some predictors
summary.mod$which
#show that R^2 for each "best" model
summary.mod$rsq
#It is not good here to use.

#show the adjusted R^2 for each "best" model
summary.mod$adjr2
#The last place have largest adjusted R^2. The model containning all the predictors.

#show the SSE for each "best" model
rss=summary.mod$rss
n2=length(Date2)
mses=c(rss[1]/(n2-2),rss[2]/(n2-3),rss[3]/(n2-4),rss[4]/(n2-5),rss[5]/(n2-6),rss[6]/(n2-7))
mses
#The last place have smallest MSE. The model containnning all the predictors.

#Using Cp Criterion to Identify "Best" models
#show that Cp criterion value for each best model
summary.mod$cp
#Therefore, expect the full model, the model with 5 predictors is the "Best" model. But in our data set, we will not use Cp to check our full model.
```

```{r}
#to plot Cook Disance
plot(fit4,which=4)
abline(h=1,lty=2)
```

```{r}
fit4=lm((response2)^(0.42)~Date2+House_age2+log(Distance2)+Store_number2+Latitude2+Longitude2)
new = data.frame(Date2=mean(Date2),House_age2=30,Distance2=mean(log(Distance2)),Store_number2=mean(Store_number2),Latitude2=mean(Latitude2),Longitude2= mean(Longitude2))
ams=predict(fit4,new, interval="predict", level=0.95,type = "response")
ams
ams^(1/0.42)
#47.25 ~ 88.48
```
```{r}
(summary(fit4))$coefficients
anova(fit4)[4]
RE2
```
